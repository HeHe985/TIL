# 워드 임베딩과 순환신경망 기반 모델(RNN & LSTM)
# 워드 임베딩
## 원-핫 인코딩
- 규칙 기반 혹은 통계적 자연어처리 연구의 대다수는 단어를 원자적(쪼갤 수 없는) 기호로 취급
- 벡터 공간 관점에서 보면, 이는 한 원소만 1이고 나머지는 모두 0인 벡터를 의미
- 차원수(=단어 사전 크기)
  - 음성 데이터(2만) - Penn Treebank(PTB) 코퍼스(5만) - big vocab(50만) - Google 1T(1300만)
  - 이를 원-핫 표현이라고 부르고, 단어를 원-핫 표현으로 바꾸는 과정을 원-핫 인코딩이라고 함
### 원-핫 인코딩의 문제점
- 예시: 웹 검색
  - [갤럭시 핸드폰] == [갤럭시 스마트폰]
- 검색 쿼리 벡터와 대상이 되는 문서 벡터들이 서로 직교하게 되어, 원-핫 벡터로는 유사도를 측정할 수 없음
1. 차원의 저주(Curse or Dimensionality)
   - 고차원의 희소 벡터를 다루기 위해선 많은 메모리 필요
   - 차원이 커질수록 데이터가 점점 더 희소해져 활용이 어려움
2. 의미적 정보 부족
   - 비슷한 단어라도 유사한 벡터로 표현되지 않음
   - "은행"과 "금융"은 의미적으로 밀접하지만, 원-핫 인코딩에서는 전혀 무관한 벡터로 취급됨
## 워드 임베딩
- 단어를 다어들 사이의 의미적 관계를 포착할 수 있는 밀집
# 순차적 데이터의 특징
# RNN
# LSTM



# MLP
# 딥러닝의 시작
- 고전적 머신러닝: 컴퓨터에게 데이터를 주고, 그 안에서 규칙을 스스로 학습하게 하는 방식
- 한계점: 이미지나 텍스트 같은 비정형 데이터 처리가 비효율적이며, 데이터 양이 많아져도 성능이 정체되는 한계
- 딥러닝의 등장: 이러한 한계를 극복하기 위해, 인간의 뇌 신경망 구조에서 영감을 받은 인공신경망 기반의 딥러닝 등장
## 퍼셉트론
- 여러 정보를 받아 최종적으로 참 또는 거짓을 결정하는 간단한 모델로, 인공신경망의 가장 기초 단위
- 원리: 각 입력값에 가중치를 곱하고 모두 더한 후 활성화 함수를 통해 최종 결과를 출력. 이는 데이터 사이에 하나의 직선을 두어 두 그룹을 나누는 것과 같음
- 가능성: AND, OR 게이트처럼 선형분리 가능한 문제들을 해결하며 기계가 스스로 규칙을 찾을 수 있다는 가능성을 보여줌
### 퍼셉트론 구성요소
- 입력(x)
- 가중치(w)
- 가중합
  - 모든 입력에 각각의 가중치를 곱한 뒤, 모두 더한 값 + 편향
  - (x₁*w₁) + (x₂*w₂) + ... + b와 같이 선형 회귀와 유사
- 활성화 함수
  - 입력된 신호를 받아, 그 신호를 처리하여 출력 신호를 결정하는 함수
  - 활성화시켜 전달할지 말지를 결정하는 스위치
### 퍼셉트론 원리
- 퍼셉트론은 결국 데이터 사이에 직선 하나를 그어서 두 그룹을 나누는 것과 같음
  - 가중합(z) = (x₁*w₁) + (x₂*w₂) + ... + b
  - z가 0보다 크면 1, 0보다 작으면 0으로 분류
  - 1차 함수를 의미
- 퍼셉트론은 선형 분리 가능한 문제를 잘 해결
- 선형 분리 가능한 문제
  - 데이터들을 하나의 직선으로 완벽하게 나눌 수 있는 문제
  - 논리 게이트(AND, OR), 간단한 분류 문제 등
### 퍼셉트론의 한계와 AI의 겨울
- 두 입력이 서로 다를 때만 1을 출력하는 XOR 문제를 하나의 직선으로는 해결 불가능
- AI의 겨울: 이 XOR 문제 때문에 퍼셉트론은 근본적인 한계가 있다는 비판을 받으며 AI연구가 침체기를 맞이
- 해결의 실마리: 퍼셉트론을 여러 층으로 쌓아 여러개의 직선을 조합하면 XOR 문제를 해결할 수 있다는 아이디어가 등장하며, 이는 신경망의 시작이 됨
# MLP의 구조와 핵심 원리
## MLP의 기본 구조
- 개념: 다층 퍼셉트론(MLP, Multi-Layer Perceptron)은 퍼셉트론이라는 기본 블록을 여러 층으로 쌓아 만든 딥러닝의 가장 기본적 모델
- 구성 요소
  - 입력층: 데이터가 처음 들어오는 곳
  - 은닉층: 입력과 출력층 사이에 1개이상 존재, 데이터의 핵심 특징을 추출하고 학습하는 가장 중요한 부분, 층이 깊어질수록 더 복잡한 패턴 학습 가능
  - 출력층: 최종 예측 결과를 내보내는 곳
## 활성화 함수
- 역할: 활성화 함수가 없다면 신경망을 아무리 깊게 쌓아도 결국 하나의 긴 선형 함수와 같음. 활성화 함수는 여기에 '비선형성'을 추가하여 복잡한 패턴을 학습할 수 있게 해주는 스위치 역할을 함
- 과거(Sigmoid): 포기에는 시그모이드 함수를 사용했지만, 역전파 시 오차 신호가 뒤로 갈수록 0에 가깝게 사라지는 **기울기 소실문제**를 발생시켜 깊은 신경망 학습을 어렵게 만들었음
- 현재(ReLU: Rectified Linear Unit): 입력이 0보다 크면 값을 그대로, 0보다 작으면 0을 출력. 기울기가 1 또는 0이므로 기울기 소실문제를 해결하고, 계산속도가 매우 빠름
# MLP학습과정
## 1단계: 순전파(Forward Propagation)
- 입력 데이터가 입력층에서 시작해 은닉층을 거쳐 출력층까지, 앞에서 뒤로 흘러가며 최종 예측값을 계산하는 과정
- 과정: 각 층의 뉴런들은 입력값과 가중치(초기에는 무작위)를 이용해 계산하고, 활성화 함수(ReLU)를 통과시켜 다음 층으로 결과 전달. 이과정이 출력층까지 반복
- 순전파 과정 상세
  1. 초기에 각 뉴런들은 무작위 가중치를 가짐
  2. 데이터가 연결된 입력층으로 들어감
  3. 각 뉴런은 입력값과 무작위로 초기화된 가중치를 이용해 계산하고, 활성화 함수를 통과시켜 결과를 다음 층으로 전달
  4. 마지막 출력층까지 반복되어 최종 예측값을 출력
#### ※ 처음에는 가중치가 무작위이므로 매우 엉터리 예측 결과가 나옴
## 2단계: 손실 함수
- 순전파로 나온 엉터리 예측 결과와 실제 정답을 비요하여 얼마나 틀렸는지(손실/오차)를 하나의 숫자로 계산
- 종류
  - 회귀: MSE
  - 분류: 교차 엔트로피 - 모델이 강하게 확신하고 틀리면 큰 벌점을 줌
- 목표
  - 손실값을 최소화하는 것이 모델 학습의 목표
## 3단계: 역전파(Back Propagation, 분석가)
- 계산된 손실의 원인을 찾아 출력층에서부터 거꾸로 추적하며, 각 가중치를 어떻게 수정해야 손실이 줄어들지 계산하는 과정
- 역전파의 원리
  1. 최종 손실에 각 뉴런이 얼마나 영향을 미쳤는지 책임을 계산
  2. 이 책임을 뒤쪽 층에서 앞쪽 층으로 계속 전달
  3. 각 가중치는 자신의 책임량에 비례하여 손실을 줄이는 방향으로 조금씩 업데이트됨
- 역전파의 진행 과정(상세)
  1. 예측값과 정답 차이에서 하나의 최종오차(L)가 발생
  2. 최종오차가 뒤로 전파되면서, 각 뉴런은 최종오차에 대한 각기 다른 책임신호를 전달받음
    - 책임신호(뉴런의 기울기 = 영향력)는 뉴련의 출력값이 최종오차에 얼마나 영향을 미쳤는지를 나타냄
    - 수학적으로는 최종오차를 각 뉴런의 출력으로 편미분
  3. 뉴런 내부의 각 (가중치 * 입력데이터)는 책임신호에 각기 다른 영향을 미치고, 이 영향력에 맞춰 가중치를 수정해야함
    - 영향력이 클 수록 가중치 수정이 더 많이 필요
  4. (가중치 * 입력데이터)의 영향력에서 실질적으로 책임신초에 영향을 주는 부분은 입력데이터
    - 가중치를 조절했을 때 결과가 얼마나 변할지가 우리의 관심사
    - 가중치의 영향력을 결정하는 것은 입력 데이터에 비례하여 결정됨
  5. 그렇기 때문에 기존의 가중치는 신경쓰지 말고, 입력데이터와 책임신호를 곱해 최종책임신호(가중치의 기울기)를 계산 수 학습률과 곱해서 각 가중치 값을 업데이트
    - 입력데이터가 증가 -> 영향력이 증가 -> 가중치의 업데이트 변화량도 커야함 -> 학습률이랑 곱해질 최종 책임신호값도 커야함
## 4단계: 옵티마이저
- 역전파로 계산된 책임(기울기)을 바탕으로 손실함수 값이 최소가 되는 최적의 가중치를 효율적으로 찾아가는 알고리즘
- 역전파를 통해 계산된 모든 가중치의 기울기와 학습률을 사용하여, 손실이 줄어드는 방향으로 실제 가중치 값을 업데이트
- 가중치 = 기존 가중치 - 학습률 * 기울기
- Adam
  - 현재 사실상의 표준 옵티마이저
  - 관성을 이용해 학습방향을 유지하고, 각 가중치마다 학습률을 적응적으로 조절하여 빠르고 안정적으로 최적의 가중치를 찾아냄
# PyTorch 코드 연계
## PyTorch와 AutoGrad
- PyTorch
  - 파이썬을 위한 대표적인 딥러닝 프레임워크
- AutoGrad
  - PyTorch의 핵심 기능으로, loss.backward() 단 한줄만 실행하면 복잡한 역전파 과정을 모두 자동으로 계산해주는 강력한 엔진
```python
# 순전파로 예측하고 손실 계산
loss = criterion(outputs, labels)

# 모든 가중치의 기울기(책임)가 자동으로 계산
loss.backward()

# 옵티마이저가 계산된 기울기를 바탕으로 모든 가중치 업데이트
optimizer.step()
```
### 모델 정의
- 모델 정의(nn.Module): __init__에서 필요한 레이어들을 정의하고 forward()에서 블록들을 어떤 순서로 조립할지(순전파)를 설계
- 데이터 준비(Dataset & DataLoader): Dataset은 전체 데이터를, DataLoader는 모델에게 데이터를 미니배치 단위로 효율적으로 공급하는 역할
```python 
class MLP(nn.Module):
  def __init__(self):
    super().__init__()
    self.net = nn.Sequential(...)   # 레고 블록 조립

  def forward(self, x):
    return self.net(x)              # 조립 순서대로 실행


train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
```
# 성능향상
## 규제
### 과적합과 규제
- 과적합
  - 모델이 훈련 데이터만 너무 열심히 외워서, 처음보는 데이터에 대한 응용력이 떨어지는 현상
- 규제
  - 과적합을 막기 위해 모델에 제약을 거는 기법
- 드롭아웃
  - 훈련시, 은닉층의 뉴런을 일정 확률로 랜덤하게 OFF 시킴
  - 이를 통해 특정 뉴런에 지나치게 의존하는 것을 방지하고, 매번 다른 네트워크를 학습시키는 듯한 **앙상블 효과**를 냄
### 훈련 종료 시점
- 학습 루프
  - 순전파 -> 손실 계산 -> 역전파 -> 가중치 업데이트 과정을 수없이 반복(Epoch)하는 것이 훈련의 전부
- 조기 종료
  - 훈련에 사용하지 않은 검증 데이터로 주기적인 모의고사를 침
  - 검증 데이터에 대한 손실이 더 이상 줄어들지 않고 오히려 증가하기 시작하면 과적합의 신호로 보고, 가장 성능이 좋았던 시점에서 학습을 자동으로 멈추는 기법

