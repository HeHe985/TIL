# 딥러닝 및 이미지 모델
# CNN
## CNN vs FCN
### FCN 모델
- 완전 연결층()
## 모델 구조
# CNN 기반 모델 변천사
## AlexNet
## VGGNet
## ResNet
## MobileNet





# 토큰화와 임베딩 - 허깅페이스 활용
# 허깅 페이스
- 최신 NLP 딥러닝 기술을 누구나 쉽게 사용할 수 있도록 도와주는 플랫폼이자 라이브러리
- 구성요소
  - Models: 전 세계 개발자들이 미리 학습시켜 둔 수만은 AI 모델들이 있음
  - Tokenizer: 각 모델에 맞는 단어 사전이 함께 비치되어 있음
  - Transformers: 이 모든 모델과 사전을 파이썬 코드로 몇줄 만에 사용할 수 있게 해주는 시스템
- 복잡한 모델을 처음부터 만들 필요없이 잘 만들어진 도구를 가져와 바로 사용 가능
# Tokenization
- 토크나이저
  - 문장을 토큰이라는 의미있는 단위로 나누고 각 토큰을 정수(ID)로 변환하는 도구
## 실습 과정
1. 토크나이저 불러오기
   - AutoTokenizer
     - 모델 이름만 알려주면, 허깅페이스에서 해당 모델에 맞는 토크나이저를 자동으로 찾아주는 클래스
   - Klue/bert-base
     - 한국어 처리를 위해 잘 학습된 모델
```python
!pip install transformers

from transformers import AutoTokenizer

# 'klue/bert-base'라는 책(모델)에 맞는 단어 사전을 빌려옵니다.
tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")

print(tokenizer)
```
2. 문장 인코딩 및 결과 분석
   - 토크나이저를 사용해 실제 문장을 컴퓨터가 이해하는 숫자로 변환
```python
text = "안녕하세요. 이 실습은 허깅페이스 토크나이저 사용법을 익히는 좋은 예제입니다."

# 토크나이저로 문장을 인코딩합니다.
encoded_input = tokenizer(text)

print(encoded_input)
# {'input_ids': [2, 5891, ...], 'token_type_ids': [...], 'attention_mask': [...]}
```
- 결과
  - input_ids: 각 토큰에 해당하는 정수 ID 리스트
  - token_type_ids, attention_mask: 여러 문장을 처리하거나, 문장 길이를 맞출 때 사용되는 추가 정보
3. 토큰 직접 확인
   - input_ids가 정말 원래 문장을 잘 표현하는지, 다시 단어로 되돌려 확인
```python
# 정수 ID를 다시 토큰으로 변환합니다.
tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'])

print(tokens)
# ['[CLS]', '안녕', '##하', '##세요', '.', ..., '[SEP]']
```
- 특징
  - [CLS], [SEP]: 문장의 시작과 끝을 알리는 특별 토큰
  - ##세요: 단어가 더 작은 의미 단위인 서브워드로 나뉜 것
    - 사전에 없는 새로운 단어가 등장해도 유연하게 대처할 수 있게 해줌(OOV 문제 해결)
## OOV(Out-of-Vocabulary)
- 모델을 학습시킬 때 사용했던 단어 사전에 존재하지 않은 새로운 단어를 의미
- 전통적인 토큰화 방식에서는 모델이 OOV단어를 만나면 그 의미를 전혀 이해하지 못하고, 보통[UNK](Unknown)이라는 하나의 특별 토큰으로 처리
- 이는 모델 성능을 저하하는 주요 원인
### OOV 문제 해결 - Subword
- 단어를 더 작은 의미 단위로 분리
- 원리: 처음 보는 단어라도 익숙한 조각들의 조합으로 만들 수 있다는 아이디어
- 모델 성능을 크게 향상시킴
# Embedding
- 토큰에 부여된 단순한 정수 ID를, 단어의 의미를 함축한 고차원의 벡터로 변환하는 과정
- 정수 ID는 숫자상으로는 가깝지만, 단순한 번호일 뿐 의미 관계를 담고 있지 못함
- 임베딩 벡터는 단어의 의미, 문맥, 관계성을 벡터 공간의 좌표값으로 표현
- 그 결과, 의미가 비슷한 단어들은 벡터 공간 상 가까운 위치에 존재, 단어 간 관계를 벡터 연산으로 표현할 수 있게 됨
## 토크나이저와 모델
- 임베딩은 모델이 수행하는 작업
- 반드시 토큰화에 사용한 토크나이저와 짝이 맞는 모델을 사용해야 함
- 다른 모델을 가져온다면, 그 모델은 단어를 전혀 다른 단어로 이해하거나 아예 모를 수 있음
## 실습 과장
1. 모델 불러오기
   - AutoModel
     - AutoTokenizer와 마찬가지로, 모델 이름을 알려주면 허깅페이스에서 해당 모델 본체를 자동으로 찾아주는 클래스
```python
from transformers import AutoModel

# 토크나이저와 '똑같은 이름'의 모델을 빌려옵니다. 짝꿍!
model = AutoModel.from_pretrained("klue/bert-base")

print(model)
```
2. 임베딩 벡터 추출하기
   - 인코딩된 input_ids를 모델에 넣어 최종 임베딩 벡터를 추출
```python
# 1. 인코딩 시, 결과를 PyTorch 텐서 형태로 반환하도록 옵션을 추가합니다.
encoded_input = tokenizer(text, return_tensors='pt')

# 2. 인코딩된 딕셔너리를 모델에 통째로 전달합니다. (**)
output = model(**encoded_input)

# 3. 모델의 여러 출력 중, 'last_hidden_state'가 우리가 원하는 최종 임베딩 벡터입니다.
embedding_vector = output.last_hidden_state

print(embedding_vector.shape) # torch.Size([1, 26, 768])
```
3. 임베딩 결과 분석
```python
print(embedding_vector.shape) # torch.Size([1, 26, 768])
```
- torch.Size([1, 26, 768])
  - 1(Batch Size): 한 번에 처리한 문장의 개수
  - 26(Sequence Length): 문장이 토큰화 된 후의 토큰 개수([CLS] ... [SEP])
  - 768(Hidden Size): 하나의 토큰을 표현하는 벡터의 차원(숫자의 개수)
- 문장을 구성하는 26개의 모든 토큰이 각각 768개의 숫자로 이루어진 의미벡터로 성공적으로 변환됨